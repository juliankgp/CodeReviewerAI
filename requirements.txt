# CodeReviewer Pro - Requirements
# Optimized for RTX 4070 (8GB VRAM) + Windows 11 + WSL2 + CUDA 11.8

# Core ML Stack - GPT-OSS-20b + LoRA + HuggingFace
torch==2.0.1+cu118
torchvision==0.15.2+cu118
torchaudio==2.0.2+cu118
transformers==4.33.2
peft==0.5.0                    # Parameter-Efficient Fine-Tuning (LoRA)
accelerate==0.23.0             # Multi-GPU and mixed precision training
bitsandbytes==0.41.1           # 8-bit and 4-bit quantization for memory efficiency

# HuggingFace Ecosystem
datasets==2.14.5              # Dataset loading and processing
tokenizers==0.13.3            # Fast tokenizers
huggingface-hub==0.17.3       # Model hub integration
safetensors==0.3.3            # Safe tensor serialization

# FastAPI Backend Stack
fastapi==0.103.1              # Modern async web framework
uvicorn[standard]==0.23.2     # ASGI server with performance optimizations
pydantic==2.4.2               # Data validation and serialization
pydantic-settings==2.0.3      # Settings management

# Database & Caching
sqlalchemy==2.0.21            # ORM for PostgreSQL
alembic==1.12.0               # Database migrations
asyncpg==0.28.4               # Async PostgreSQL driver
redis==5.0.0                  # Redis client for caching
hiredis==2.2.3                # High performance Redis parser

# Authentication & Security
python-jose[cryptography]==3.3.0  # JWT tokens
passlib[bcrypt]==1.7.4         # Password hashing
python-multipart==0.0.6       # Form data parsing

# Monitoring & Observability
prometheus-client==0.17.1     # Metrics collection
structlog==23.1.0             # Structured logging
opencensus-ext-prometheus==0.8.0  # OpenTelemetry metrics

# Development & Testing
pytest==7.4.2                 # Testing framework
pytest-asyncio==0.21.1        # Async test support
pytest-cov==4.1.0             # Coverage reporting
httpx==0.25.0                  # HTTP client for testing
factory-boy==3.3.0            # Test data generation

# Code Quality
black==23.9.1                 # Code formatting
isort==5.12.0                 # Import sorting
flake8==6.1.0                 # Linting
mypy==1.6.1                   # Type checking
pre-commit==3.4.0             # Git hooks

# Jupyter & Data Science (for ML experiments)
jupyter==1.0.0                # Jupyter notebooks
ipywidgets==8.1.1             # Interactive widgets
matplotlib==3.7.2             # Plotting
seaborn==0.12.2               # Statistical plotting
pandas==2.1.1                 # Data manipulation
numpy==1.25.2                 # Numerical computing
scikit-learn==1.3.0           # Classical ML algorithms

# GPU Monitoring & Optimization
nvidia-ml-py==12.535.108      # NVIDIA GPU monitoring
py3nvml==0.2.7                # NVIDIA management library

# Utilities
python-dotenv==1.0.0          # Environment variable loading
click==8.1.7                  # CLI framework
rich==13.6.0                  # Rich text and beautiful formatting
tqdm==4.66.1                  # Progress bars
requests==2.31.0              # HTTP requests
aiofiles==23.2.1              # Async file operations
python-slugify==8.0.1         # URL slug generation
email-validator==2.0.0        # Email validation
bcrypt==4.0.1                 # Password hashing backend

# Memory Management & Performance
psutil==5.9.5                 # System and process monitoring
memory-profiler==0.61.0       # Memory usage profiling
line-profiler==4.1.1          # Line-by-line profiling

# Production Dependencies
gunicorn==21.2.0              # WSGI server for production
supervisor==4.2.5             # Process management

# RTX 4070 (8GB) Specific Optimizations:
# - torch with CUDA 11.8 support for maximum compatibility
# - bitsandbytes for aggressive 4-bit quantization
# - accelerate for mixed precision training
# - CUDA memory management optimized for 8GB VRAM constraint
# - Async libraries for non-blocking I/O during inference

# Memory Usage Estimates (RTX 4070 8GB):
# - GPT-OSS-20b (fp16): ~40GB -> Requires aggressive quantization
# - GPT-OSS-20b (4-bit): ~10GB -> Exceeds 8GB, need smaller model or 8-bit
# - GPT-OSS-7b (4-bit): ~3.5GB -> Optimal for 8GB VRAM
# - LoRA adapters: ~200MB per task
# - Inference batch size: 1 sample optimal for memory
# - Training batch size: 1 sample for LoRA fine-tuning
# - Recommendation: Use GPT-OSS-7b or CodeLlama-7b for optimal performance